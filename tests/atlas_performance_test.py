"""
ATLAS Model Performance Benchmark
æµ‹è¯•æ¨¡å‹çš„TFlopsæ€§èƒ½ï¼ŒåŒ…æ‹¬ç†è®ºå’Œå®é™…æ€§èƒ½
"""

import time
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Tuple, List, Dict
import gc
import psutil
import GPUtil
from contextlib import contextmanager
import warnings
import sys
warnings.filterwarnings("ignore")

# å¯¼å…¥æ¨¡å‹æ¶æ„ï¼ˆéœ€è¦ä»atlas2.pyå¯¼å…¥ï¼‰
from src.atlas2 import ATLASModel, create_specialized_kernels
print("âœ… Successfully imported ATLAS model")

class PerformanceBenchmark:
    """ATLASæ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self, model_path: str = "../models/atlas_binary_model_best.pth"):
        self.model_path = model_path
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        # self.device = torch.device("cpu")
        self.model = None
        self.compiled_model = None
        
        print(f"ğŸš€ Initializing Performance Benchmark")
        print(f"ğŸ“± Device: {self.device}")
        print(f"ğŸ”§ PyTorch Version: {torch.__version__}")
        
        if torch.cuda.is_available():
            print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
            print(f"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        
    def load_model(self) -> None:
        """åŠ è½½æ¨¡å‹"""
        try:
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            kernels = create_specialized_kernels()
            self.model = ATLASModel(
                input_shape=(50, 50, 4),
                kernels=kernels,
                dropout_rate=0.5
            ).to(self.device)
            
            # åŠ è½½æƒé‡
            state_dict = torch.load(self.model_path, map_location=self.device)
            self.model.load_state_dict(state_dict)
            self.model.eval()
            
            # ç¼–è¯‘æ¨¡å‹ï¼ˆPyTorch 2.0+ï¼‰
            # if hasattr(torch, 'compile'):
            if False:
                print("ğŸ”§ Compiling model with torch.compile()...")
                self.compiled_model = torch.compile(self.model, mode='max-autotune')
            else:
                print("âš ï¸  torch.compile not available, using standard model")
                self.compiled_model = self.model
                
            print("âœ… Model loaded successfully")
            self._print_model_info()
            
        except Exception as e:
            print(f"âŒ Error loading model: {e}")
            raise

    def _print_model_info(self) -> None:
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"\nğŸ“Š Model Information:")
        print(f"   Total Parameters: {total_params:,}")
        print(f"   Trainable Parameters: {trainable_params:,}")
        print(f"   Model Size: {self._get_model_size_mb():.2f} MB")

    def _get_model_size_mb(self) -> float:
        """è®¡ç®—æ¨¡å‹å¤§å°ï¼ˆMBï¼‰"""
        param_size = 0
        buffer_size = 0
        
        for param in self.model.parameters():
            param_size += param.nelement() * param.element_size()
        
        for buffer in self.model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()
            
        model_size = (param_size + buffer_size) / 1024 / 1024
        return model_size

    def calculate_theoretical_flops(self, input_shape: Tuple[int, int, int, int]) -> int:
        """è®¡ç®—ç†è®ºFLOPs"""
        try:
            # å°è¯•ä½¿ç”¨thopåº“
            from thop import profile
            dummy_input = torch.randn(input_shape).to(self.device)
            flops, params = profile(self.model, inputs=(dummy_input,), verbose=False)
            return int(flops)
        except ImportError:
            # æ‰‹åŠ¨ä¼°ç®—FLOPs
            batch_size, channels, height, width = input_shape
            
            # ä¼°ç®—å„ä¸ªç»„ä»¶çš„FLOPs
            # Conv2d: output_elements * (kernel_size^2 * input_channels + 1)
            # Linear: input_features * output_features
            
            estimated_flops = 0
            
            # 4ä¸ªåˆ†æ”¯ï¼Œæ¯ä¸ªåˆ†æ”¯ä¸¤ä¸ªå·ç§¯å±‚
            for _ in range(4):
                # ç¬¬ä¸€ä¸ªå·ç§¯å±‚ï¼šå‡è®¾è¾“å‡º32ä¸ªé€šé“ï¼Œ5x5å·ç§¯æ ¸
                conv1_flops = (height * width) * (5 * 5 * 1 + 1) * 9  # å¹³å‡9ä¸ªæ ¸
                # æ± åŒ–åå°ºå¯¸å‡åŠ
                height_pool = height // 2
                width_pool = width // 2
                # ç¬¬äºŒä¸ªå·ç§¯å±‚ï¼š32ä¸ªè¾“å‡ºé€šé“ï¼Œ3x3å·ç§¯æ ¸
                conv2_flops = (height_pool * width_pool) * (3 * 3 * 9 + 1) * 32
                estimated_flops += conv1_flops + conv2_flops
            
            # åˆ†ç±»å™¨å±‚
            classifier_flops = (128 * 64) + (64 * 32) + (32 * 1)
            estimated_flops += classifier_flops
            
            # ä¹˜ä»¥batch size
            estimated_flops *= batch_size
            
            print(f"ğŸ“Š Using estimated FLOPs calculation: {estimated_flops:,}")
            return estimated_flops

    @contextmanager
    def timer(self):
        """è®¡æ—¶å™¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        start = time.perf_counter()
        yield
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        end = time.perf_counter()
        self.elapsed_time = end - start

    def warmup(self, input_shape: Tuple[int, int, int, int], warmup_iterations: int = 10) -> None:
        """æ¨¡å‹é¢„çƒ­"""
        print(f"ğŸ”¥ Warming up model ({warmup_iterations} iterations)...")
        
        with torch.no_grad():
            for _ in range(warmup_iterations):
                dummy_input = torch.randn(input_shape).to(self.device)
                _ = self.compiled_model(dummy_input)
                
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("âœ… Warmup completed")

    def benchmark_inference(
        self, 
        batch_sizes: List[int] = [1, 4, 8, 16, 32],
        input_size: Tuple[int, int] = (50, 50),
        num_iterations: int = 100
    ) -> Dict[str, List[float]]:
        """æ¨ç†æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print(f"\nğŸƒâ€â™‚ï¸ Starting inference benchmark...")
        print(f"   Batch sizes: {batch_sizes}")
        print(f"   Input size: {input_size}")
        print(f"   Iterations per batch: {num_iterations}")
        
        results = {
            'batch_sizes': [],
            'avg_latency_ms': [],
            'throughput_samples_per_sec': [],
            'theoretical_tflops': [],
            'actual_tflops': [],
            'gpu_utilization': [],
            'memory_usage_mb': []
        }
        
        for batch_size in batch_sizes:
            print(f"\nğŸ“Š Testing batch size: {batch_size}")
            
            input_shape = (batch_size, 4, input_size[0], input_size[1])
            
            # é¢„çƒ­
            self.warmup(input_shape, warmup_iterations=5)
            
            # è®¡ç®—ç†è®ºFLOPs
            theoretical_flops = self.calculate_theoretical_flops(input_shape)
            
            # æ€§èƒ½æµ‹è¯•
            latencies = []
            
            with torch.no_grad():
                for i in range(num_iterations):
                    # ç”Ÿæˆéšæœºè¾“å…¥
                    dummy_input = torch.randn(input_shape).to(self.device)
                    
                    # è®°å½•GPUä½¿ç”¨æƒ…å†µï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if torch.cuda.is_available() and i == 0:
                        gpu_before = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                    
                    # è®¡æ—¶æ¨ç†
                    with self.timer():
                        outputs = self.compiled_model(dummy_input)
                    
                    latencies.append(self.elapsed_time * 1000)  # Convert to ms
                    
                    if i == 0 and torch.cuda.is_available():
                        gpu_after = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                        memory_usage = gpu_after - gpu_before
                    
                    # è¿›åº¦æ˜¾ç¤º
                    if (i + 1) % 20 == 0:
                        print(f"   Progress: {i + 1}/{num_iterations}")
            
            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            avg_latency = np.mean(latencies[10:])  # å¿½ç•¥å‰10æ¬¡ä»¥é¿å…åˆå§‹åŒ–å¼€é”€
            std_latency = np.std(latencies[10:])
            
            throughput = batch_size / (avg_latency / 1000)  # samples per second
            actual_tflops = (theoretical_flops / (avg_latency / 1000)) / 1e12
            
            # GPUä½¿ç”¨ç‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            gpu_util = 0
            if torch.cuda.is_available():
                try:
                    gpus = GPUtil.getGPUs()
                    if gpus:
                        gpu_util = gpus[0].load * 100
                except:
                    gpu_util = 0
            
            # å­˜å‚¨ç»“æœ
            results['batch_sizes'].append(batch_size)
            results['avg_latency_ms'].append(avg_latency)
            results['throughput_samples_per_sec'].append(throughput)
            results['theoretical_tflops'].append(theoretical_flops / 1e12)
            results['actual_tflops'].append(actual_tflops)
            results['gpu_utilization'].append(gpu_util)
            results['memory_usage_mb'].append(memory_usage if torch.cuda.is_available() else 0)
            
            # æ‰“å°ç»“æœ
            print(f"   â±ï¸  Avg Latency: {avg_latency:.2f} Â± {std_latency:.2f} ms")
            print(f"   ğŸš€ Throughput: {throughput:.1f} samples/sec")
            print(f"   âš¡ TFlops: {actual_tflops:.3f}")
            print(f"   ğŸ’¾ Memory: {memory_usage if torch.cuda.is_available() else 0:.1f} MB")
            
            # æ¸…ç†å†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
        
        return results

    def compare_compiled_vs_standard(
        self, 
        batch_size: int = 16, 
        num_iterations: int = 50
    ) -> Dict[str, float]:
        """æ¯”è¾ƒç¼–è¯‘æ¨¡å‹vsæ ‡å‡†æ¨¡å‹æ€§èƒ½"""
        print(f"\nâš”ï¸  Comparing compiled vs standard model...")
        
        input_shape = (batch_size, 4, 50, 50)
        
        # æµ‹è¯•æ ‡å‡†æ¨¡å‹
        print("ğŸ”§ Testing standard model...")
        self.model.eval()
        latencies_standard = []
        
        with torch.no_grad():
            for _ in range(num_iterations):
                dummy_input = torch.randn(input_shape).to(self.device)
                with self.timer():
                    _ = self.model(dummy_input)
                latencies_standard.append(self.elapsed_time * 1000)
        
        # æµ‹è¯•ç¼–è¯‘æ¨¡å‹
        print("âš¡ Testing compiled model...")
        latencies_compiled = []
        
        with torch.no_grad():
            for _ in range(num_iterations):
                dummy_input = torch.randn(input_shape).to(self.device)
                with self.timer():
                    _ = self.compiled_model(dummy_input)
                latencies_compiled.append(self.elapsed_time * 1000)
        
        avg_standard = np.mean(latencies_standard[5:])  # å¿½ç•¥å‰5æ¬¡
        avg_compiled = np.mean(latencies_compiled[5:])
        speedup = avg_standard / avg_compiled
        
        results = {
            'standard_latency_ms': avg_standard,
            'compiled_latency_ms': avg_compiled,
            'speedup': speedup
        }
        
        print(f"   ğŸ“Š Standard Model: {avg_standard:.2f} ms")
        print(f"   âš¡ Compiled Model: {avg_compiled:.2f} ms")
        print(f"   ğŸš€ Speedup: {speedup:.2f}x")
        
        return results

    def create_performance_report(self, results: Dict, comparison: Dict) -> None:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Šå¯è§†åŒ–"""
        print("\nğŸ“ˆ Generating performance report...")
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('ATLAS Model Performance Benchmark Report', fontsize=16, fontweight='bold')
        
        # 1. å»¶è¿Ÿ vs æ‰¹æ¬¡å¤§å°
        axes[0, 0].plot(results['batch_sizes'], results['avg_latency_ms'], 'bo-', linewidth=2, markersize=8)
        axes[0, 0].set_xlabel('Batch Size')
        axes[0, 0].set_ylabel('Average Latency (ms)')
        axes[0, 0].set_title('Latency vs Batch Size')
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. ååé‡ vs æ‰¹æ¬¡å¤§å°
        axes[0, 1].plot(results['batch_sizes'], results['throughput_samples_per_sec'], 'go-', linewidth=2, markersize=8)
        axes[0, 1].set_xlabel('Batch Size')
        axes[0, 1].set_ylabel('Throughput (samples/sec)')
        axes[0, 1].set_title('Throughput vs Batch Size')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. TFlops vs æ‰¹æ¬¡å¤§å°
        axes[0, 2].plot(results['batch_sizes'], results['actual_tflops'], 'ro-', linewidth=2, markersize=8)
        axes[0, 2].set_xlabel('Batch Size')
        axes[0, 2].set_ylabel('TFlops')
        axes[0, 2].set_title('Computational Performance (TFlops)')
        axes[0, 2].grid(True, alpha=0.3)
        
        # 4. å†…å­˜ä½¿ç”¨
        if torch.cuda.is_available():
            axes[1, 0].bar(results['batch_sizes'], results['memory_usage_mb'], color='purple', alpha=0.7)
            axes[1, 0].set_xlabel('Batch Size')
            axes[1, 0].set_ylabel('Memory Usage (MB)')
            axes[1, 0].set_title('GPU Memory Usage')
            axes[1, 0].grid(True, alpha=0.3)
        else:
            axes[1, 0].text(0.5, 0.5, 'GPU not available', ha='center', va='center', transform=axes[1, 0].transAxes)
            axes[1, 0].set_title('GPU Memory Usage')
        
        # 5. ç¼–è¯‘å¯¹æ¯”
        comparison_data = [comparison['standard_latency_ms'], comparison['compiled_latency_ms']]
        comparison_labels = ['Standard', 'Compiled']
        bars = axes[1, 1].bar(comparison_labels, comparison_data, color=['lightblue', 'lightgreen'])
        axes[1, 1].set_ylabel('Latency (ms)')
        axes[1, 1].set_title(f'Standard vs Compiled\n(Speedup: {comparison["speedup"]:.2f}x)')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, comparison_data):
            axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                           f'{value:.1f}ms', ha='center', va='bottom')
        
        # 6. æ•ˆç‡åˆ†æï¼ˆç†è®ºvså®é™…TFlopsï¼‰
        efficiency = [actual/theoretical*100 for actual, theoretical in 
                     zip(results['actual_tflops'], results['theoretical_tflops'])]
        axes[1, 2].plot(results['batch_sizes'], efficiency, 'mo-', linewidth=2, markersize=8)
        axes[1, 2].set_xlabel('Batch Size')
        axes[1, 2].set_ylabel('Efficiency (%)')
        axes[1, 2].set_title('Computational Efficiency\n(Actual/Theoretical TFlops)')
        axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('atlas_performance_report.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("âœ… Performance report saved as 'atlas_performance_report.png'")

    def print_summary_report(self, results: Dict, comparison: Dict) -> None:
        """æ‰“å°æ€§èƒ½æ€»ç»“æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ† ATLAS MODEL PERFORMANCE SUMMARY REPORT")
        print("="*80)
        
        print(f"\nğŸ“Š BASIC INFORMATION:")
        print(f"   ğŸ”§ Device: {self.device}")
        print(f"   ğŸ“± Model Parameters: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"   ğŸ’¾ Model Size: {self._get_model_size_mb():.2f} MB")
        
        print(f"\nâš¡ PERFORMANCE HIGHLIGHTS:")
        max_tflops_idx = np.argmax(results['actual_tflops'])
        max_throughput_idx = np.argmax(results['throughput_samples_per_sec'])
        min_latency_idx = np.argmin(results['avg_latency_ms'])
        
        print(f"   ğŸš€ Peak TFlops: {results['actual_tflops'][max_tflops_idx]:.3f} (batch size {results['batch_sizes'][max_tflops_idx]})")
        print(f"   ğŸƒ Max Throughput: {results['throughput_samples_per_sec'][max_throughput_idx]:.1f} samples/sec (batch size {results['batch_sizes'][max_throughput_idx]})")
        print(f"   â±ï¸  Min Latency: {results['avg_latency_ms'][min_latency_idx]:.2f} ms (batch size {results['batch_sizes'][min_latency_idx]})")
        
        print(f"\nğŸ”§ TORCH.COMPILE BENEFITS:")
        print(f"   âš¡ Speedup: {comparison['speedup']:.2f}x")
        print(f"   ğŸ“‰ Latency Reduction: {((comparison['standard_latency_ms'] - comparison['compiled_latency_ms']) / comparison['standard_latency_ms'] * 100):.1f}%")
        
        print(f"\nğŸ“ˆ SCALABILITY ANALYSIS:")
        efficiency_range = [min(results['actual_tflops']), max(results['actual_tflops'])]
        print(f"   ğŸ“Š TFlops Range: {efficiency_range[0]:.3f} - {efficiency_range[1]:.3f}")
        print(f"   ğŸ“ˆ Batch Scaling: {results['throughput_samples_per_sec'][-1]/results['throughput_samples_per_sec'][0]:.1f}x improvement (BS1â†’BS{results['batch_sizes'][-1]})")
        
        # æ¨èçš„ä½¿ç”¨é…ç½®
        balanced_idx = len(results['batch_sizes']) // 2  # ä¸­ç­‰batch sizeé€šå¸¸æœ€å¹³è¡¡
        print(f"\nğŸ’¡ RECOMMENDED CONFIGURATION:")
        print(f"   ğŸ¯ Optimal Batch Size: {results['batch_sizes'][max_tflops_idx]} (for max TFlops)")
        print(f"   âš–ï¸  Balanced Configuration: Batch Size {results['batch_sizes'][balanced_idx]} ")
        print(f"      - Latency: {results['avg_latency_ms'][balanced_idx]:.2f} ms")
        print(f"      - Throughput: {results['throughput_samples_per_sec'][balanced_idx]:.1f} samples/sec")
        print(f"      - TFlops: {results['actual_tflops'][balanced_idx]:.3f}")
        
        print("\n" + "="*80)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ATLAS Model Performance Benchmark")
    print("="*50)
    
    # åˆå§‹åŒ–åŸºå‡†æµ‹è¯•
    benchmark = PerformanceBenchmark("models/atlas_binary_model_best.pth")
    
    # åŠ è½½æ¨¡å‹
    benchmark.load_model()
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    results = benchmark.benchmark_inference(
        batch_sizes=[1, 2, 4, 8, 16, 32],
        input_size=(50, 50),
        num_iterations=100
    )
    
    # æ¯”è¾ƒç¼–è¯‘vsæ ‡å‡†æ¨¡å‹
    comparison = benchmark.compare_compiled_vs_standard(batch_size=16, num_iterations=50)
    
    # ç”ŸæˆæŠ¥å‘Š
    benchmark.create_performance_report(results, comparison)
    benchmark.print_summary_report(results, comparison)
    
    print("\nğŸ‰ Benchmark completed successfully!")


if __name__ == "__main__":
    main()
